{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import transforms # import transformations to use for demo\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/Activation-functions-examples-pytorch/raw/master/assets/background-card-chip.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[_Photo by Fancycrave.com from Pexels_](https://www.pexels.com/photo/green-ram-card-collection-825262/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# In-Place Operations in PyTorch\n",
    "_What are they and why avoid them_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's advanced deep neural networks have millions of parameters (for example, see the comparison in [this paper](https://arxiv.org/pdf/1905.11946.pdf)) and trying to train them on free GPU's like Kaggle or Goggle Colab often leads to running out of memory on GPU. There are several simple ways to reduce the GPU memory occupied by the model, for example:\n",
    "* Consider changing the architecture of the model or using the type of model with fewer parameters (for example choose [DenseNet](https://arxiv.org/pdf/1608.06993.pdf)-121 over DenseNet-169). This approach can affect model's performance metrics.\n",
    "* Reduce the batch size or manually set the number of data loader workers. In this case it will take longer for the model to train.\n",
    "\n",
    "Using in-place operations in neural networks may help to avoid the downsides of approaches mentioned above while saving some GPU memory. However it is strongly __not recommended to use in-place operations__ for several reasons.\n",
    "\n",
    "In this kernel I would like to:\n",
    "* Describe what are the in-place operations and demonstrate how they might help to save the GPU memory.\n",
    "* Tell why we should avoid the in-place operations or use them with great caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place Operations\n",
    "`In-place operation is an operation that changes directly the content of a given linear algebra, vector, matrices(Tensor) without making a copy. The operators which helps to do the operation is called in-place operator.` See the [tutorial](https://www.tutorialspoint.com/inplace-operator-in-python) on in-place operations in Python.\n",
    "\n",
    "As it is said in the definition, in-place operations don't make a copy of the input, that is why they can help to reduce the memory usage, when operating with high-dimentional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to run a simple model on [Fashion MNIST dataset](https://www.kaggle.com/zalando-research/fashionmnist) to demonstrate how in-place operations help to consume less GPU memory. In this demonstration I use simple fully-connected deep neural network with four linear layers and [ReLU](https://pytorch.org/docs/stable/nn.html#relu) activation after each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeting Up The Demo\n",
    "Ih this section I will prepare everything for the demonstration:\n",
    "* Load Fashion MNIST dataset from PyTorch,\n",
    "* Introduce transformations for Fashion MNIST images using PyTorch,\n",
    "* Prepare model training procedure.\n",
    "\n",
    "If you are familiar with PyTorch basics, just skip this part and go straight to the rest of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to transform the input data is to use buil-in PyTorch transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data I used standard Dataset and Dataloader classes from PyTorch and [FashionMNIST class code from this kernel](https://www.kaggle.com/arturlacerda/pytorch-conditional-gan):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    '''\n",
    "    Dataset clas to load Fashion MNIST data from csv.\n",
    "    Code from original kernel:\n",
    "    https://www.kaggle.com/arturlacerda/pytorch-conditional-gan\n",
    "    '''\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        fashion_df = pd.read_csv('../input/fashion-mnist_train.csv')\n",
    "        self.labels = fashion_df.label.values\n",
    "        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Load the training data for Fashion MNIST\n",
    "trainset = FashionMNIST(transform=transform)\n",
    "# Define the dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Procedure\n",
    "I wrote a small training procedure, which runs 5 training epochs and prints the loss for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device):\n",
    "    '''\n",
    "    Function trains the model and prints out the training log.\n",
    "    '''\n",
    "    #setup training\n",
    "    \n",
    "    #define loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "    #define learning rate\n",
    "    learning_rate = 0.003\n",
    "    #define number of epochs\n",
    "    epochs = 5\n",
    "    #initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    #run training and print out the loss to make sure that we are actually fitting to the training set\n",
    "    print('Training the model \\n')\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            # print out the loss to make sure it is decreasing\n",
    "            print(f\"Training loss: {running_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "PyTorch provides us with in-place implementation of ReLU activation function. I will run consequently training with in-place ReLU implementation and with vanilla ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for basic fully-connected deep neural network\n",
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "    Demo classifier model class to demonstrate in-place operations\n",
    "    '''\n",
    "    def __init__(self, inplace = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = inplace) # pass inplace as parameter to ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure the input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # apply activation function\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        # apply activation function\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        # apply activation function\n",
    "        x = self.relu(self.fc3(x))\n",
    "        \n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Memory Usage for In-place and Vanilla Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare memory usage for one single call of ReLU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty caches and setup the device\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_allocated(device, inplace = False):\n",
    "    '''\n",
    "    Function measures allocated memory before and after the ReLU function call.\n",
    "    '''\n",
    "    \n",
    "    # Create a large tensor\n",
    "    t = torch.randn(10000, 10000, device=device)\n",
    "    \n",
    "    # Measure allocated memory\n",
    "    torch.cuda.synchronize()\n",
    "    start_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    start_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    # Call in-place or normal ReLU\n",
    "    if inplace:\n",
    "        F.relu_(t)\n",
    "    else:\n",
    "        output = F.relu(t)\n",
    "    \n",
    "    # Measure allocated memory after the call\n",
    "    torch.cuda.synchronize()\n",
    "    end_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    end_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    # Return amount of memory allocated for ReLU call\n",
    "    return end_memory - start_memory, end_max_memory - start_max_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run out of place ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 382.0\n",
      "Allocated max memory: 382.0\n"
     ]
    }
   ],
   "source": [
    "memory_allocated, max_memory_allocated = get_memory_allocated(device, inplace = False)\n",
    "print('Allocated memory: {}'.format(memory_allocated))\n",
    "print('Allocated max memory: {}'.format(max_memory_allocated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in-place ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.0\n",
      "Allocated max memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "memory_allocated_inplace, max_memory_allocated_inplace = get_memory_allocated(device, inplace = True)\n",
    "print('Allocated memory: {}'.format(memory_allocated_inplace))\n",
    "print('Allocated max memory: {}'.format(max_memory_allocated_inplace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same while training a simple classifier.\n",
    "Run training with vanilla ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model \n",
      "\n",
      "Training loss: 490.5504989773035\n",
      "Training loss: 361.1345275044441\n",
      "Training loss: 329.05726308375597\n",
      "Training loss: 306.97832968086004\n",
      "Training loss: 292.6471059694886\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "model = Classifier(inplace = False)\n",
    "\n",
    "# measure allocated memory\n",
    "torch.cuda.synchronize()\n",
    "start_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "start_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "# train the classifier\n",
    "train_model(model, device)\n",
    "\n",
    "# measure allocated memory after training\n",
    "torch.cuda.synchronize()\n",
    "end_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "end_memory = torch.cuda.memory_allocated() / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 1.853515625\n",
      "Allocated max memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Allocated memory: {}'.format(end_memory - start_memory))\n",
    "print('Allocated max memory: {}'.format(end_max_memory - start_max_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training with in-place ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model \n",
      "\n",
      "Training loss: 485.5531446188688\n",
      "Training loss: 359.61066341400146\n",
      "Training loss: 329.1772850751877\n",
      "Training loss: 307.14213905483484\n",
      "Training loss: 292.3229675516486\n"
     ]
    }
   ],
   "source": [
    "# initialize model with in-place ReLU\n",
    "model = Classifier(inplace = True)\n",
    "\n",
    "# measure allocated memory\n",
    "torch.cuda.synchronize()\n",
    "start_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "start_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "# train the classifier with in-place ReLU\n",
    "train_model(model, device)\n",
    "\n",
    "# measure allocated memory after training\n",
    "torch.cuda.synchronize()\n",
    "end_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "end_memory = torch.cuda.memory_allocated() / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 1.853515625\n",
      "Allocated max memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Allocated memory: {}'.format(end_memory - start_memory))\n",
    "print('Allocated max memory: {}'.format(end_max_memory - start_max_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like using in-place ReLU really helps us to save some GPU memory. But we should be __extremely cautious when using in-place operations and check twice__. In the next section I will show why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsides of In-place Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major downside of in-place operations is the fact that __they might overwrite values required to compute gradients__ which means breaking the training procedure of the model. That is what [the official PyTorch autograd documentation](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd) says:\n",
    "> Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\n",
    "\n",
    "> There are two main reasons that limit the applicability of in-place operations:\n",
    "\n",
    "> 1. In-place operations can potentially overwrite values required to compute gradients.\n",
    "> 2. Every in-place operation actually requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the Function representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other reason of being careful with in-place operations is that their implementation is extremely tricky. That is why I would __recommend to use PyTorch standard in-place operations__  (like `torch.tanh_` or `torch.sigmoid_`) instead of implementing one manually.\n",
    "\n",
    "Let's see an example of [SiLU](https://arxiv.org/pdf/1606.08415.pdf) (or Swish-1) activation function. This is the normal implementation of SiLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu(input):\n",
    "    '''\n",
    "    Normal implementation of SiLU activation function\n",
    "    https://arxiv.org/pdf/1606.08415.pdf\n",
    "    '''\n",
    "    return input * torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement in-place SiLU using torch.sigmoid_ in-place function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu_inplace_1(input):\n",
    "    '''\n",
    "    Incorrect implementation of in-place SiLU activation function\n",
    "    https://arxiv.org/pdf/1606.08415.pdf\n",
    "    '''\n",
    "    return input * torch.sigmoid_(input) # THIS IS INCORRECT!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above __incorrectly__ implements in-place SiLU. We can make sure of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SiLU: tensor([ 0.0796, -0.2744, -0.2598])\n",
      "In-place SiLU: tensor([0.5370, 0.2512, 0.2897])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3)\n",
    "\n",
    "# print result of original SiLU\n",
    "print(\"Original SiLU: {}\".format(silu(t)))\n",
    "\n",
    "# change the value of t with in-place function\n",
    "silu_inplace_1(t)\n",
    "print(\"In-place SiLU: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that the function `silu_inplace_1` in fact returns `sigmoid(input) * sigmoid(input)` !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working example of the in-place implementation of SiLU using `torch.sigmoid_` could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu_inplace_2(input):\n",
    "    '''\n",
    "    Example of implementation of in-place SiLU activation function using torch.sigmoid_\n",
    "    https://arxiv.org/pdf/1606.08415.pdf\n",
    "    '''\n",
    "    result = input.clone()\n",
    "    torch.sigmoid_(input)\n",
    "    input *= result\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SiLU: tensor([ 0.7774, -0.2767,  0.2967])\n",
      "In-place SiLU #2: tensor([ 0.7774, -0.2767,  0.2967])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3)\n",
    "\n",
    "# print result of original SiLU\n",
    "print(\"Original SiLU: {}\".format(silu(t)))\n",
    "\n",
    "# change the value of t with in-place function\n",
    "silu_inplace_2(t)\n",
    "print(\"In-place SiLU #2: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small example demonstrates why we should be extremely careful and check twice when using the in-place operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this article: \n",
    "* I described the in-place operations and their purpose. Demonstrated how in-place operations help to __consume less GPU memory__.\n",
    "* Described the major __downsides of in-place operations__. One should be very careful about using them and check the result twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "Links to the additional resources and further reading:\n",
    "\n",
    "1. [PyTorch Autograd documentation](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS\n",
    "![echo logo](https://github.com/Lexie88rus/Activation-functions-examples-pytorch/blob/master/assets/echo_logo.png?raw=true)\n",
    "\n",
    "I participate in implementation of a __Echo package__ with mathematical backend for neural networks, which can be used with most popular existing packages (TensorFlow, Keras and [PyTorch](https://pytorch.org/)). We have done a lot for PyTorch and Keras so far. Here is a [link to a repository on GitHub](https://github.com/digantamisra98/Echo/tree/Dev-adeis), __I will highly appreciate your feedback__ on that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
